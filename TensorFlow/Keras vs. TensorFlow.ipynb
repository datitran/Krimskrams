{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras vs. TensorFlow Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "pip install keras\n",
    "\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = np.random.randn(100)\n",
    "y = 0.5 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 0s - loss: 0.7871 - val_loss: 0.4989\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s - loss: 0.4099 - val_loss: 0.2602\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s - loss: 0.2140 - val_loss: 0.1357\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s - loss: 0.1114 - val_loss: 0.0708\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s - loss: 0.0582 - val_loss: 0.0370\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s - loss: 0.0304 - val_loss: 0.0193\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s - loss: 0.0159 - val_loss: 0.0101\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0052\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s - loss: 0.0023 - val_loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.50820935]], dtype=float32), array([ 0.96137643], dtype=float32)]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, activation=\"linear\", input_dim=1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "model.summary()\n",
    "model.fit(x, y, batch_size=5, validation_split=0.2, verbose=1)\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "def progressbar(it, size=30):\n",
    "    count = len(it)\n",
    "    def _show(_i):\n",
    "        x = int(size*_i/count)\n",
    "        sys.stdout.write(\"{}/{} [{}{}] \\r\".format(_i, count, \"=\"*x, \".\"*(size-x)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    _show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        _show(i+1)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "full_size = 80\n",
    "batch_size = 5\n",
    "-> full gradient pass = 16 (80/5)\n",
    "\"\"\"\n",
    "for i in progressbar(range(80)):\n",
    "    if i > 16:\n",
    "        # just run the progessbar without any changes\n",
    "        pass\n",
    "    else:\n",
    "        # do something\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "80/80 [==============================] \n",
      "loss: 0.11840341240167618 - val_loss: 0.07780846208333969\n",
      "Epoch: 2/10\n",
      "80/80 [==============================] \n",
      "loss: 0.0613195076584816 - val_loss: 0.029116839170455933\n",
      "Epoch: 3/10\n",
      "80/80 [==============================] \n",
      "loss: 0.01843276061117649 - val_loss: 0.005032940302044153\n",
      "Epoch: 4/10\n",
      "80/80 [==============================] \n",
      "loss: 0.004215208347886801 - val_loss: 0.0013901116326451302\n",
      "Epoch: 5/10\n",
      "80/80 [==============================] \n",
      "loss: 0.0004166064318269491 - val_loss: 0.00045925291487947106\n",
      "Epoch: 6/10\n",
      "80/80 [==============================] \n",
      "loss: 0.00014951403136365116 - val_loss: 0.00013726545148529112\n",
      "Epoch: 7/10\n",
      "80/80 [==============================] \n",
      "loss: 7.649453618796542e-05 - val_loss: 3.642608135123737e-05\n",
      "Epoch: 8/10\n",
      "80/80 [==============================] \n",
      "loss: 1.6197009244933724e-05 - val_loss: 1.188176975119859e-05\n",
      "Epoch: 9/10\n",
      "80/80 [==============================] \n",
      "loss: 8.289369361591525e-06 - val_loss: 3.334345137773198e-06\n",
      "Epoch: 10/10\n",
      "80/80 [==============================] \n",
      "loss: 1.8179456446887343e-06 - val_loss: 1.117770580094657e-06\n",
      "Optimization Finished!\n",
      "Training cost= 7.84454e-07 W= 0.499193 b= 0.999621 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "split = 0.2\n",
    "batch_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholder & Variables\n",
    "    X = tf.placeholder(tf.float32, name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "    # tf.random_normal(shape=[]) == np.random.randn()\n",
    "    W = tf.Variable(tf.random_normal(shape=[]), name=\"weight\")\n",
    "    b = tf.Variable(tf.random_normal(shape=[]), name=\"bias\")\n",
    "\n",
    "    # Construct a linear model\n",
    "    Y_predicted = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "    # Mean squared error\n",
    "    #cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "    cost = tf.losses.mean_squared_error(labels=Y, predictions=Y_predicted)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph) # python -m tensorflow.tensorboard logs=\"./graphs\"\n",
    "    sess.run(init)\n",
    "    \n",
    "    # validation_split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=split)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, training_epochs))\n",
    "        \n",
    "        train_range = round(x_train.shape[0] / batch_size)\n",
    "        for index in progressbar(range(x_train.shape[0])):\n",
    "            if index > train_range:\n",
    "                pass\n",
    "            else:\n",
    "                mini_batch = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "                _, loss = sess.run([optimizer, cost], feed_dict={X: x_train[mini_batch], Y: y_train[mini_batch]})\n",
    "        \n",
    "        val_range = round(x_val.shape[0] / batch_size)\n",
    "        for _ in range(x_val.shape[0]): \n",
    "            mini_batch = np.random.choice(x_val.shape[0], batch_size, replace=False)\n",
    "            _, val_loss = sess.run([optimizer, cost], feed_dict={X: x_val[mini_batch], Y: y_val[mini_batch]})\n",
    "            \n",
    "        print(\"loss: {} - val_loss: {}\".format(loss, val_loss))\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: x, Y: y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- Early Stopping can be implemented via ValidationMonitor but rather used TFLearn\n",
    "- TFLearn on top of TF; alternative is TF-Slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation=\"softmax\", input_shape=(784,)))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 0s - loss: 1.3845 - acc: 0.6689 - val_loss: 0.8994 - val_acc: 0.8246\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.7938 - acc: 0.8276 - val_loss: 0.6604 - val_acc: 0.8584\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.6435 - acc: 0.8508 - val_loss: 0.5637 - val_acc: 0.8703\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.5709 - acc: 0.8608 - val_loss: 0.5102 - val_acc: 0.8792\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.5267 - acc: 0.8685 - val_loss: 0.4756 - val_acc: 0.8834\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4963 - acc: 0.8730 - val_loss: 0.4511 - val_acc: 0.8871\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4739 - acc: 0.8778 - val_loss: 0.4330 - val_acc: 0.8893\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4565 - acc: 0.8806 - val_loss: 0.4185 - val_acc: 0.8920\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4425 - acc: 0.8829 - val_loss: 0.4068 - val_acc: 0.8941\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4309 - acc: 0.8849 - val_loss: 0.3971 - val_acc: 0.8971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123e5bdd8>"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_full, y_train_full = x_train[:1000], y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "800/800 [==============================] \n",
      "loss: 2.243738889694214 - acc: 0.6171875 - val_loss: 0.9551129341125488 - val_acc: 0.890625\n",
      "Epoch: 2/10\n",
      "800/800 [==============================] \n",
      "loss: 1.046567678451538 - acc: 0.8203125 - val_loss: 0.6591847538948059 - val_acc: 0.921875\n",
      "Epoch: 3/10\n",
      "800/800 [==============================] \n",
      "loss: 0.9539118409156799 - acc: 0.734375 - val_loss: 0.4772654175758362 - val_acc: 0.9296875\n",
      "Epoch: 4/10\n",
      "800/800 [==============================] \n",
      "loss: 0.8135160803794861 - acc: 0.8125 - val_loss: 0.4343830645084381 - val_acc: 0.921875\n",
      "Epoch: 5/10\n",
      "800/800 [==============================] \n",
      "loss: 0.7396941781044006 - acc: 0.828125 - val_loss: 0.32984787225723267 - val_acc: 0.96875\n",
      "Epoch: 6/10\n",
      "800/800 [==============================] \n",
      "loss: 0.7155982255935669 - acc: 0.8046875 - val_loss: 0.28279608488082886 - val_acc: 0.9921875\n",
      "Epoch: 7/10\n",
      "800/800 [==============================] \n",
      "loss: 0.8052401542663574 - acc: 0.765625 - val_loss: 0.280452162027359 - val_acc: 0.984375\n",
      "Epoch: 8/10\n",
      "800/800 [==============================] \n",
      "loss: 0.6108319163322449 - acc: 0.828125 - val_loss: 0.22645746171474457 - val_acc: 0.9921875\n",
      "Epoch: 9/10\n",
      "800/800 [==============================] \n",
      "loss: 0.7159842252731323 - acc: 0.7890625 - val_loss: 0.21547859907150269 - val_acc: 1.0\n",
      "Epoch: 10/10\n",
      "800/800 [==============================] \n",
      "loss: 0.6562367677688599 - acc: 0.8046875 - val_loss: 0.18838568031787872 - val_acc: 0.9921875\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 128\n",
    "split = 0.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    Y_predicted = tf.add(tf.matmul(X, W),  b)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=Y_predicted))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    # evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_predicted, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph) # python -m tensorflow.tensorboard logs=\"./graphs\"\n",
    "    sess.run(init)\n",
    "    \n",
    "    # validation_split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=split)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, training_epochs))\n",
    "        \n",
    "        train_range = round(x_train.shape[0] / batch_size)\n",
    "        for index in progressbar(range(x_train.shape[0])):\n",
    "            if index > train_range:\n",
    "                pass\n",
    "            else:\n",
    "                mini_batch = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "                _, loss, acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_train[mini_batch], Y: y_train[mini_batch]})\n",
    "\n",
    "        \n",
    "        val_range = round(x_val.shape[0] / batch_size)\n",
    "        for _ in range(x_val.shape[0]): \n",
    "            mini_batch = np.random.choice(x_val.shape[0], batch_size, replace=False)\n",
    "            _, val_loss, val_acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_val[mini_batch], Y: y_val[mini_batch]})\n",
    "            \n",
    "        print(\"loss: {} - acc: {} - val_loss: {} - val_acc: {}\".format(loss, acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation=\"relu\", input_shape=(784,)))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s - loss: 1.5283 - acc: 0.5766 - val_loss: 0.9033 - val_acc: 0.8153\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.7361 - acc: 0.8290 - val_loss: 0.5626 - val_acc: 0.8661\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.5407 - acc: 0.8630 - val_loss: 0.4557 - val_acc: 0.8836\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4622 - acc: 0.8769 - val_loss: 0.4039 - val_acc: 0.8940\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.4189 - acc: 0.8856 - val_loss: 0.3735 - val_acc: 0.8978\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.3906 - acc: 0.8914 - val_loss: 0.3526 - val_acc: 0.9039\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.3705 - acc: 0.8967 - val_loss: 0.3369 - val_acc: 0.9064\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.3554 - acc: 0.9000 - val_loss: 0.3254 - val_acc: 0.9083\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.3429 - acc: 0.9035 - val_loss: 0.3162 - val_acc: 0.9110\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 0s - loss: 0.3329 - acc: 0.9059 - val_loss: 0.3078 - val_acc: 0.9123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118269198>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_full, y_train_full = x_train[:1000], y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "800/800 [==============================] \n",
      "loss: 2.357222318649292 - acc: 0.1484375 - val_loss: 2.2194454669952393 - val_acc: 0.28125\n",
      "Epoch: 2/10\n",
      "800/800 [==============================] \n",
      "loss: 2.257216453552246 - acc: 0.2421875 - val_loss: 2.149240016937256 - val_acc: 0.4140625\n",
      "Epoch: 3/10\n",
      "800/800 [==============================] \n",
      "loss: 2.1923890113830566 - acc: 0.359375 - val_loss: 2.0849528312683105 - val_acc: 0.4140625\n",
      "Epoch: 4/10\n",
      "800/800 [==============================] \n",
      "loss: 2.0928196907043457 - acc: 0.40625 - val_loss: 1.9503047466278076 - val_acc: 0.546875\n",
      "Epoch: 5/10\n",
      "800/800 [==============================] \n",
      "loss: 2.0281476974487305 - acc: 0.453125 - val_loss: 1.8639016151428223 - val_acc: 0.6796875\n",
      "Epoch: 6/10\n",
      "800/800 [==============================] \n",
      "loss: 1.9074482917785645 - acc: 0.625 - val_loss: 1.762959599494934 - val_acc: 0.671875\n",
      "Epoch: 7/10\n",
      "800/800 [==============================] \n",
      "loss: 1.7573871612548828 - acc: 0.6484375 - val_loss: 1.6267309188842773 - val_acc: 0.765625\n",
      "Epoch: 8/10\n",
      "800/800 [==============================] \n",
      "loss: 1.6207945346832275 - acc: 0.6796875 - val_loss: 1.511582612991333 - val_acc: 0.7734375\n",
      "Epoch: 9/10\n",
      "800/800 [==============================] \n",
      "loss: 1.5492334365844727 - acc: 0.6953125 - val_loss: 1.4011660814285278 - val_acc: 0.7421875\n",
      "Epoch: 10/10\n",
      "800/800 [==============================] \n",
      "loss: 1.3828023672103882 - acc: 0.8125 - val_loss: 1.2676076889038086 - val_acc: 0.8046875\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 128\n",
    "split = 0.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    def init_weights(shape):\n",
    "        weights = tf.random_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(weights)\n",
    "\n",
    "    def deep_neural_network(X, w_1, w_2):\n",
    "        h    = tf.nn.sigmoid(tf.matmul(X, w_1)) \n",
    "        yhat = tf.matmul(h, w_2)\n",
    "        return yhat\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    w_1 = init_weights((784, 32))\n",
    "    w_2 = init_weights((32, 10))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    Y_predicted = deep_neural_network(X, w_1, w_2)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=Y_predicted))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    # evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_predicted, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph) # python -m tensorflow.tensorboard logs=\"./graphs\"\n",
    "    sess.run(init)\n",
    "    \n",
    "    # validation_split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=split)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, training_epochs))\n",
    "        \n",
    "        train_range = round(x_train.shape[0] / batch_size)\n",
    "        for index in progressbar(range(x_train.shape[0])):\n",
    "            if index > train_range:\n",
    "                pass\n",
    "            else:\n",
    "                mini_batch = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "                _, loss, acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_train[mini_batch], Y: y_train[mini_batch]})\n",
    "\n",
    "        \n",
    "        val_range = round(x_val.shape[0] / batch_size)\n",
    "        for _ in range(x_val.shape[0]): \n",
    "            mini_batch = np.random.choice(x_val.shape[0], batch_size, replace=False)\n",
    "            _, val_loss, val_acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_val[mini_batch], Y: y_val[mini_batch]})\n",
    "            \n",
    "        print(\"loss: {} - acc: {} - val_loss: {} - val_acc: {}\".format(loss, acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_train /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), strides=(1, 1), activation=\"relu\", input_shape=(28, 28, 1), padding=\"same\")) # same -> input = output\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, (5, 5), strides=(1, 1), activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 1s - loss: 1.9131 - val_loss: 1.7521ss: 1.\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 1s - loss: 1.5303 - val_loss: 1.3653\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 1s - loss: 1.1207 - val_loss: 0.9838\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 2s - loss: 0.8032 - val_loss: 0.7448\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 2s - loss: 0.6500 - val_loss: 0.6654\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 2s - loss: 0.5567 - val_loss: 0.6286\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 2s - loss: 0.5007 - val_loss: 0.5702\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 2s - loss: 0.4536 - val_loss: 0.5901\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 2s - loss: 0.4886 - val_loss: 0.5311\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 2s - loss: 0.3710 - val_loss: 0.5424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118d6a358>"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train[:1000], y_train[:1000], batch_size=32, epochs=10, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_full = x_train[:1000]\n",
    "y_train_full = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "800/800 [==============================] \n",
      "loss: 1.0250860452651978 - acc: 0.6875 - val_loss: 0.016407014802098274 - val_acc: 1.0\n",
      "Epoch: 2/10\n",
      "800/800 [==============================] \n",
      "loss: 0.22624161839485168 - acc: 0.90625 - val_loss: 0.00957321748137474 - val_acc: 1.0\n",
      "Epoch: 3/10\n",
      "800/800 [==============================] \n",
      "loss: 0.19930750131607056 - acc: 0.96875 - val_loss: 0.004512276966124773 - val_acc: 1.0\n",
      "Epoch: 4/10\n",
      "800/800 [==============================] \n",
      "loss: 0.07598122954368591 - acc: 1.0 - val_loss: 0.003296941053122282 - val_acc: 1.0\n",
      "Epoch: 5/10\n",
      "800/800 [==============================] \n",
      "loss: 0.17946262657642365 - acc: 0.9375 - val_loss: 0.0025500478222966194 - val_acc: 1.0\n",
      "Epoch: 6/10\n",
      "800/800 [==============================] \n",
      "loss: 0.036343879997730255 - acc: 1.0 - val_loss: 0.0017508012242615223 - val_acc: 1.0\n",
      "Epoch: 7/10\n",
      "800/800 [==============================] \n",
      "loss: 0.11032424122095108 - acc: 0.96875 - val_loss: 0.0011565235909074545 - val_acc: 1.0\n",
      "Epoch: 8/10\n",
      "800/800 [==============================] \n",
      "loss: 0.11011669039726257 - acc: 0.96875 - val_loss: 0.0017978237010538578 - val_acc: 1.0\n",
      "Epoch: 9/10\n",
      "800/800 [==============================] \n",
      "loss: 0.4545377492904663 - acc: 0.96875 - val_loss: 0.002106127794831991 - val_acc: 1.0\n",
      "Epoch: 10/10\n",
      "800/800 [==============================] \n",
      "loss: 0.07084725052118301 - acc: 0.96875 - val_loss: 0.0010846415534615517 - val_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024]) # need to understand how paddle and stride works to define the numbers\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    Y_predicted = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_predicted), reduction_indices=[1]))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_predicted, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"./graphs\", sess.graph) # python -m tensorflow.tensorboard logs=\"./graphs\"\n",
    "    sess.run(init)\n",
    "    \n",
    "    # validation_split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=split)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, training_epochs))\n",
    "        \n",
    "        train_range = round(x_train.shape[0] / batch_size)\n",
    "        for index in progressbar(range(x_train.shape[0])):\n",
    "            if index > train_range:\n",
    "                pass\n",
    "            else:\n",
    "                mini_batch = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
    "                _, loss, acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_train[mini_batch], Y: y_train[mini_batch]})\n",
    "\n",
    "        \n",
    "        val_range = round(x_val.shape[0] / batch_size)\n",
    "        for _ in range(x_val.shape[0]): \n",
    "            mini_batch = np.random.choice(x_val.shape[0], batch_size, replace=False)\n",
    "            _, val_loss, val_acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={X: x_val[mini_batch], Y: y_val[mini_batch]})\n",
    "            \n",
    "        print(\"loss: {} - acc: {} - val_loss: {} - val_acc: {}\".format(loss, acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
